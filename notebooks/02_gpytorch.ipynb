{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45430f69",
   "metadata": {},
   "source": [
    "\n",
    "# Gpytorch / Pylightcurve-torch tutorial\n",
    "Gpytorch is ...\n",
    "\n",
    "GPs for exo-transits ...\n",
    "\n",
    "Pros of using GPytorch:\n",
    "\n",
    "- Exact or approximate GPs\n",
    "- Deep GPs\n",
    "- Integration of Neural Nets moduls with Pytorch\n",
    "- Scalable, flexible...\n",
    "\n",
    "In this notebook, simple examples of use of GP along with Pylightcurve-torch transit module.\n",
    "Examples adapted from [GPytorch tutorials](https://docs.gpytorch.ai/en/v1.1.1/examples/01_Exact_GPs/Simple_GP_Regression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905d88f",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "- Proper Parameters (hyperparameters) estimation ! \n",
    "- solve the nans problem with kepler data --> solved, but should we introduce constraints?\n",
    "- Run on GPUs\n",
    "- Scaling, how does that work for long LCs?\n",
    "- which params to fit? (not really the quetion here but...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from pylightcurve_torch import TransitModule\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ca9a3",
   "metadata": {},
   "source": [
    "# Toy Light Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce451030",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 200).double().detach()  # Time vector\n",
    "\n",
    "# Transit model\n",
    "pars =  {'e':0.01, 'i':90., 'w':0., 'rp': 0.1, 'method': 'linear', 'ldc': [0.1], \n",
    "             'P': 4., 't0': 0.6, 'a': 7., 'fp': 0.0001}\n",
    "tm = TransitModule(train_x, **pars, )\n",
    "\n",
    "# True function is sin(2*pi*x) with Gaussian noise and transit\n",
    "train_data = 100. + (torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * np.sqrt(0.01)) / 2\n",
    "train_data *= tm()[0]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train_x, train_data.detach())\n",
    "plt.title('Data to fit')\n",
    "plt.ylabel('Flux')\n",
    "plt.xlabel('Time (days)')\n",
    "\n",
    "# Standardise\n",
    "mu = train_data.mean()\n",
    "sigma = train_data.std()\n",
    "\n",
    "# mu = 0\n",
    "# sigma = 1\n",
    "\n",
    "preprocess = lambda y: (y - mu) / sigma\n",
    "postprocess = lambda y: y * sigma + mu\n",
    "# preprocess = lambda y: y\n",
    "train_y = preprocess(train_data)\n",
    "train_y.double().detach_()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d528a4",
   "metadata": {},
   "source": [
    "### Define GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tm.set_params(**pars)\n",
    "tm.set_params(rp=0.2, t0=0.55)\n",
    "tm.activate_grad('rp','t0')   # which transit params to fit\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, tm):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean() \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.transit_module = tm \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = (mu + self.mean_module(x)) \n",
    "        mean_x *= self.transit_module(time=x.T)[0]  # Careful to the input here, time has to be last dim not first\n",
    "        mean_x = preprocess(mean_x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "        \n",
    "    \n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().double()\n",
    "model = ExactGPModel(train_x, train_y, likelihood, tm).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f757f",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_iter = 100\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': list(model.mean_module.parameters()) \n",
    "               + list(model.covar_module.parameters())\n",
    "               + list(model.likelihood.parameters())},\n",
    "    {'params': list(model.transit_module.parameters()), 'lr':0.01},  #### different LR for transit par\n",
    "], lr=0.1)  # Default LR?\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "# Optimise the GP model\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.5f  rp: %.5f  t0: %.5f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item(),\n",
    "        model.transit_module.rp.item(),\n",
    "        model.transit_module.t0.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80c7ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#     test_x = torch.linspace(0, 1, 301).double()\n",
    "    test_x = train_x \n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13686f0e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(4, 1, figsize=(12, 12), sharex=True)\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax[0].plot(train_x.numpy(), train_y.numpy(), 'k*', label='Observed Data')\n",
    "    # Plot predictive means as blue line\n",
    "#     ax[0].plot(test_x.numpy(), preprocess(postprocess(observed_pred.mean) / model.transit_module(time=test_x)[0]), 'r', label='stellar component of the mean')\n",
    "    ax[0].plot(test_x.numpy(), observed_pred.mean.numpy(), 'b', label='Mean')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax[0].fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5, label='Confidence')\n",
    "    #ax.set_ylim([-3, 3])\n",
    "    ax[0].set_ylabel('normalised flux')\n",
    "    ax[0].set_title('Fitted data and model')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].plot(test_x.numpy(), postprocess(observed_pred.mean) / model.transit_module(time=test_x)[0])\n",
    "    ax[1].set_title('Stellar component')\n",
    "    ax[1].set_ylabel('Flux')\n",
    "\n",
    "    ax[2].plot(test_x.numpy(), model.transit_module(time=test_x)[0].detach())\n",
    "    ax[2].set_title('Transit model')\n",
    "    ax[2].set_ylabel('stellar flux units')\n",
    "    \n",
    "    ax[3].scatter(test_x.numpy(), postprocess(train_y - observed_pred.mean))\n",
    "    ax[3].set_title('Residuals')\n",
    "    ax[3].set_ylabel('Flux')\n",
    "    ax[3].set_xlabel('Time (days)')\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da538b",
   "metadata": {},
   "source": [
    "# Tess Light Curve\n",
    "\n",
    "What's the point here: scalability? fitting more params? real data? \n",
    "\n",
    "Section could well be ditched otherwise, for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a21e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightkurve as lk\n",
    "lc_search = lk.search_lightcurve('kepler-1b')[0]\n",
    "lc = lc_search.download()\n",
    "lc.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e856af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = lc[(lc.time.value>122.) & (lc.time.value<126.)]\n",
    "lc.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(lc.time.value.astype(np.float64))\n",
    "train_y = torch.tensor(lc.flux.value.astype(np.float64))\n",
    "\n",
    "# Transit model\n",
    "pars =  {'e':0., 'i':83.8, 'w':0., 'rp': 0.2, 'method': 'linear', 'ldc': [0.1], \n",
    "         'P': 2.4706132, 't0': 122.71, 'a': 7.9, 'fp': 0.0001}\n",
    "\n",
    "tm = TransitModule(train_x, **pars)\n",
    "\n",
    "# med_flux = np.nanmedian(train_y.detach().numpy())\n",
    "\n",
    "mu = np.nanmean(train_y)\n",
    "sigma = np.nanstd(train_y)\n",
    "preprocess = lambda y: (y - mu) / sigma\n",
    "postprocess = lambda y: y * sigma + mu\n",
    "\n",
    "train_y = preprocess(train_y)\n",
    "train_y[np.isnan(train_y)] = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8230da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tm.set_params(**pars)\n",
    "tm.set_params(rp=0.2, t0=122.75) #, a=30, ldc=0.2)\n",
    "tm.activate_grad('rp','t0', 'P')   # which transit params to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13527a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, tm):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean() \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.transit_module = tm \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = (mu + self.mean_module(x)) \n",
    "        mean_x *= self.transit_module(time=x.T)[0]  # Careful to the input here, time has to be last dim not first\n",
    "        mean_x = preprocess(mean_x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().double()\n",
    "model = ExactGPModel(train_x, train_y, likelihood, tm).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7af480",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': list(model.mean_module.parameters()) \n",
    "               + list(model.covar_module.parameters())\n",
    "               + list(model.likelihood.parameters())},\n",
    "    {'params': list(model.transit_module.parameters()), 'lr':0.005},  #### different LR for transit par\n",
    "], lr=0.01)  # Default LR?\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# Optimise the GP model\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.5f  rp: %.5f  t0: %.5f P: %.5f  ' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item(),\n",
    "        model.transit_module.rp.item(),\n",
    "        model.transit_module.t0.item(),\n",
    "        model.transit_module.P.item(),\n",
    "#         model.transit_module.ldc.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267aee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(train_x[0], train_x[-1], 301).double()\n",
    "    test_x = train_x\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(4, 1, figsize=(12, 12), sharex=True)\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax[0].plot(train_x.numpy(), train_y.numpy(), 'k*', label='Observed Data')\n",
    "    # Plot predictive means as blue line\n",
    "#     ax[0].plot(test_x.numpy(), preprocess(postprocess(observed_pred.mean) / model.transit_module(time=test_x)[0]), 'r', label='stellar component of the mean')\n",
    "    ax[0].plot(test_x.numpy(), observed_pred.mean.numpy(), 'b', label='Mean')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax[0].fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5, label='Confidence')\n",
    "    #ax.set_ylim([-3, 3])\n",
    "    ax[0].set_ylabel('normalised flux')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].plot(test_x.numpy(), postprocess(observed_pred.mean) / model.transit_module(time=test_x)[0])\n",
    "    ax[1].set_title('Stellar component')\n",
    "    ax[1].set_ylabel('Flux')\n",
    "\n",
    "    ax[2].plot(test_x.numpy(), model.transit_module(time=test_x)[0].detach())\n",
    "    ax[2].set_title('Transit model')\n",
    "    ax[2].set_ylabel('stellar flux units')\n",
    "    \n",
    "    ax[3].scatter(test_x.numpy(), postprocess(train_y - observed_pred.mean), s=3)\n",
    "    ax[3].set_title('Residuals')\n",
    "    ax[3].set_ylabel('Flux')\n",
    "    ax[3].set_xlabel('Time (days)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plct-develop)",
   "language": "python",
   "name": "plct-develop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
